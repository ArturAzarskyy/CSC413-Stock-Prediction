{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_prepros.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPo/BzSGSTtLfDO8XsrVGiV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArturAzarskyy/CSC413-Stock-Prediction/blob/main/Transformer/transformer_prepros.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This is the preprocessing code for our modles\n",
        "\n",
        "Note that some parts of ideas and code was taken form the Jan Schmitz notebook on IBM stock prediciton [IBM stock predictor](https://github.com/JanSchm/CapMarket/blob/master/bot_experiments/IBM_Transformer%2BTimeEmbedding.ipynb). Though Jan S. worked only with one stock we extended the idea to multiple stocks as well as used different dataset. We are also looking at a bit different model as well.\n"
      ],
      "metadata": {
        "id": "jhuhIQFovDqx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QlDcGQ5Gx6B_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e35799e-a360-4351-96dc-44ac660a4bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n",
            "amex-nyse-nasdaq-stock-histories.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  amex-nyse-nasdaq-stock-histories.zip\n",
            "replace all_symbols.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!echo '{\"username\":\"arturusmaximus\",\"key\":\"4f14194978499e9ae1ad6adb74b94add\"}' > /content/kaggle.json\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "# !kaggle datasets download -d borismarjanovic/price-volume-data-for-all-us-stocks-etfs\n",
        "!kaggle datasets download -d qks1lver/amex-nyse-nasdaq-stock-histories\n",
        "# !unzip price-volume-data-for-all-us-stocks-etfs.zip\n",
        "!unzip amex-nyse-nasdaq-stock-histories.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch as ty\n",
        "import os.path\n",
        "import numpy as np\n",
        "\n",
        "time_step = 3\n",
        "stock_histoy_lenght = 128 # this does not include the label\n",
        "# stock_histoy_lenght = 63 # this does not include the label\n",
        "f_train_data   = open(\"f_train_data.npy\", 'ab')\n",
        "f_train_labels = open(\"f_train_labels.npy\", 'ab')\n",
        "f_val_data     = open(\"f_val_data.npy\", 'ab')\n",
        "f_val_labels   = open(\"f_val_labels.npy\", 'ab')\n",
        "f_test_data    = open(\"f_test_data.npy\", 'ab')\n",
        "f_test_labels  = open(\"f_test_labels.npy\", 'ab')\n"
      ],
      "metadata": {
        "id": "uDf-WA6-xi7I"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdb\n",
        "# train_data = None\n",
        "# train_labels = None\n",
        "# val_data = None\n",
        "# val_labels = None\n",
        "# test_data = None\n",
        "# test_labels = None\n",
        "\n",
        "with open('all_symbols.txt') as topo_file:\n",
        "    for line in topo_file:\n",
        "        if os.path.isfile(\"full_history/\"+line[:-1]+\".csv\"):\n",
        "            df = pd.read_csv(\"full_history/\"+line[:-1]+\".csv\",\n",
        "                             delimiter=',', \n",
        "                             usecols=['date', 'open', 'high', 'low', 'close', 'volume'])\n",
        "            \n",
        "            df['volume'].replace(to_replace=0, method='ffill', inplace=True) \n",
        "            df['open'] = df['open'].pct_change()\n",
        "            df['high'] = df['high'].pct_change()\n",
        "            df['low'] = df['low'].pct_change()\n",
        "            df['close'] = df['close'].pct_change()\n",
        "            df['volume'] = df['volume'].pct_change()\n",
        "\n",
        "            df.dropna(how='any', axis=0, inplace=True)\n",
        "            if len(df.index.values) == 0:\n",
        "                continue\n",
        "            df.sort_values('date', inplace=True)\n",
        "            df = df.reset_index(drop=True)\n",
        "            valid_start = sorted(df.index.values)[-int(0.3*df.shape[0])]  \n",
        "            test_start = sorted(df.index.values)[-int(0.1*df.shape[0])]\n",
        "\n",
        "            if int(0.2*df.shape[0]) < stock_histoy_lenght+1 or int(0.1*df.shape[0]) < stock_histoy_lenght+1:\n",
        "                continue\n",
        "\n",
        "            # Normalizing the data\n",
        "\n",
        "            min_return = min(df[(df.index < valid_start)][['open', 'high', 'low', 'close']].min(axis=0))\n",
        "            max_return = max(df[(df.index < valid_start)][['open', 'high', 'low', 'close']].max(axis=0))\n",
        "\n",
        "            df['open'] = (df['open'] - min_return) / (max_return - min_return)\n",
        "            df['high'] = (df['high'] - min_return) / (max_return - min_return)\n",
        "            df['low'] =  (df['low'] - min_return) / (max_return - min_return)\n",
        "            df['close']= (df['close'] - min_return) / (max_return - min_return)\n",
        "\n",
        "            min_volume = df[(df.index < valid_start)]['volume'].min(axis=0)\n",
        "            max_volume = df[(df.index < valid_start)]['volume'].max(axis=0)\n",
        "\n",
        "            df['volume'] = (df['volume'] - min_volume) / (max_volume - min_volume)\n",
        "\n",
        "\n",
        "            df.drop(columns=['date'], inplace=True)\n",
        "            df_train = df[(df.index < valid_start)]\n",
        "            df_val = df[(df.index >= valid_start) & (df.index < test_start)]\n",
        "            df_test = df[(df.index >= test_start)]\n",
        "            update_train_data = []\n",
        "            update_train_labels = []\n",
        "            update_val_data = []\n",
        "            update_val_labels = []\n",
        "            update_test_data = []\n",
        "            update_test_labels = []\n",
        "\n",
        "\n",
        "            c_train_data = df_train.values\n",
        "            c_val_data = df_val.values\n",
        "            c_test_data = df_test.values\n",
        "            for i in range(stock_histoy_lenght, len(c_train_data), time_step):\n",
        "                update_train_data.append(c_train_data[i-stock_histoy_lenght:i])\n",
        "                update_train_labels.append(c_train_data[i, 3])\n",
        "                # train_data.append(c_train_data[i-stock_histoy_lenght:i])\n",
        "                # train_labels.append(c_train_data[i, 3])\n",
        "            for i in range(stock_histoy_lenght, len(c_val_data), time_step):\n",
        "                update_val_data.append(c_val_data[i-stock_histoy_lenght:i])\n",
        "                update_val_labels.append(c_val_data[i, 3])\n",
        "                # val_data.append(c_val_data[i-stock_histoy_lenght:i])\n",
        "                # val_labels.append(c_val_data[i, 3])\n",
        "            for i in range(stock_histoy_lenght, len(c_test_data), time_step):\n",
        "                update_test_data.append(c_test_data[i-stock_histoy_lenght:i])\n",
        "                update_test_labels.append(c_test_data[i, 3])\n",
        "                # test_data.append(c_test_data[i-stock_histoy_lenght:i])\n",
        "                # test_labels.append(c_test_data[i, 3])\n",
        "            np.save(f_train_data,np.array(update_train_data))\n",
        "            np.save(f_train_labels,np.array(update_train_labels))\n",
        "            np.save(f_val_data, np.array(update_val_data))\n",
        "            np.save(f_val_labels, np.array(update_val_labels))\n",
        "            np.save(f_test_data, np.array(update_test_data))\n",
        "            np.save(f_test_labels, np.array(update_test_labels))\n",
        "            # if train_data is None:\n",
        "            #     train_data = np.array(update_train_data)\n",
        "            #     train_labels = np.array(update_train_labels)\n",
        "            # else:\n",
        "            #     train_data = np.concatenate((train_data, update_train_data), axis=0)\n",
        "            #     train_labels = np.concatenate((train_labels, update_train_labels), axis=0)\n",
        "\n",
        "            # if val_data is None:\n",
        "            #     val_data =  np.array(update_val_data)\n",
        "            #     val_labels = np.array(update_val_labels)\n",
        "            # else:\n",
        "            #     val_data = np.concatenate((val_data, update_val_data), axis=0)\n",
        "            #     val_labels = np.concatenate((val_labels, update_val_labels), axis=0)\n",
        "\n",
        "            # if test_data is None:\n",
        "            #     test_data = np.array(update_test_data)\n",
        "            #     test_labels = np.array(update_test_labels)\n",
        "            # else:\n",
        "            #     test_data = np.concatenate((test_data, update_test_data), axis=0)\n",
        "            #     test_labels = np.concatenate((test_labels, update_test_labels), axis=0)\n",
        "\n"
      ],
      "metadata": {
        "id": "2uouYOPwyLKZ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f_train_data.close()\n",
        "f_train_labels.close()\n",
        "f_val_data.close()\n",
        "f_val_labels.close()\n",
        "f_test_data.close()\n",
        "f_test_labels.close()"
      ],
      "metadata": {
        "id": "69xRL9Dx1FKu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data   = np.load(\"f_train_data.npy\", mmap_mode='r')\n",
        "train_labels = np.load(\"f_train_labels.npy\", mmap_mode='r')\n",
        "val_data     = np.load(\"f_val_data.npy\", mmap_mode='r')\n",
        "val_labels   = np.load(\"f_val_labels.npy\", mmap_mode='r')\n",
        "test_data    = np.load(\"f_test_data.npy\", mmap_mode='r')\n",
        "test_labels  = np.load(\"f_test_labels.npy\", mmap_mode='r')"
      ],
      "metadata": {
        "id": "z-VNomVS1XVP"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQjEdUpDQyNR",
        "outputId": "254c71e5-a6b7-4e90-eb02-1bbb67dcd06d"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(472, 128, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = np.array(train_data)\n",
        "train_labels = np.array(train_labels)"
      ],
      "metadata": {
        "id": "uak0dLnbQ4yw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.array(train_data), np.array(train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "pXFl3m4jOQlI",
        "outputId": "ec47d078-e007-4c2e-b988-71de14b221b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-bc4400526542>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, train_labels = np.array(train_data), np.array(train_labels)\n",
        "val_data, val_labels = np.array(val_data), np.array(val_labels)\n",
        "test_data, test_labels = np.array(test_data), np.array(test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "qyht95YvxZ-I",
        "outputId": "a0526076-d7be-4fe9-e457-646066d123d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9adeafd7cde3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Pv97-0CrzB8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "np.save(\"stocks_train_data_no_m_avg\", train_data)\n",
        "np.save(\"stocks_train_labels_no_m_avg\", train_labels)\n",
        "np.save(\"stocks_val_data_no_m_avg\", val_data)\n",
        "np.save(\"stocks_val_labels_no_m_avg\", val_labels)\n",
        "np.save(\"stocks_test_data_no_m_avg\", test_data)\n",
        "np.save(\"stocks_test_labels_no_m_avg\", test_labels)"
      ],
      "metadata": {
        "id": "G768ism4xcKl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}