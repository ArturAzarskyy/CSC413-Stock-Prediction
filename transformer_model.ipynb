{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6xZQTf6vGy0T3w8wuSo7z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArturAzarskyy/CSC413-Stock-Prediction/blob/main/transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model for Stock prediction"
      ],
      "metadata": {
        "id": "9pLJEKQAho8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preporations:\n",
        "\n",
        "Note that torch dataset  and sampler were inspired from `yousefnami`'s article about\n",
        "[Reading .h5 Files Faster with PyTorch Datasets](https://towardsdatascience.com/reading-h5-files-faster-with-pytorch-datasets-3ff86938cc)"
      ],
      "metadata": {
        "id": "JjaPnVGE9pSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the pre-processed data from the "
      ],
      "metadata": {
        "id": "SlAbOtnh1p-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/amd/')"
      ],
      "metadata": {
        "id": "EunBcf6jhomO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "518a213b-2a71-43d6-c854-96e619b9e593"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /amd/; to attempt to forcibly remount, call drive.mount(\"/amd/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_mvg_avg_f = False"
      ],
      "metadata": {
        "id": "Kq3zQAVM2S04"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if load_mvg_avg_f:\n",
        "    !cp /amd/My\\ Drive/CSC413/Data/sp_data_orig_m_avg.zip /content/\n",
        "    !unzip sp_data_orig.zip\n",
        "else:\n",
        "    !cp /amd/My\\ Drive/CSC413/Data/sp_data_orig.zip /content/\n",
        "    !unzip sp_data_orig.zip"
      ],
      "metadata": {
        "id": "YoOJi5YUjUbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c39fc93a-8a41-48fb-8d4a-d6b53dd8488f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  sp_data_orig.zip\n",
            "replace test_data.hdf5? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yEFAqEAX_5S3"
      },
      "outputs": [],
      "source": [
        "import tables\n",
        "import torch as ty\n",
        "import os.path\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a custom dataset for pytorch"
      ],
      "metadata": {
        "id": "sn4tkdqM1ep4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler\n",
        "from torchvision.transforms import Compose\n",
        "import tables\n",
        "\n",
        "\n",
        "class StockDataset(Dataset):\n",
        "    def __init__(self, file_name, shuffle=True):\n",
        "        super(StockDataset, self).__init__()\n",
        "        hdf5_file = tables.open_file(file_name, mode='r')\n",
        "        assert('data' in hdf5_file.root)\n",
        "        assert('labels' in hdf5_file.root)\n",
        "        self.f_name = file_name\n",
        "        self.data = hdf5_file.root.data\n",
        "        self.lables = hdf5_file.root.labels\n",
        "        self.size = self.data.shape[0]\n",
        "        self.shuffle = shuffle\n",
        "        self.trans_data = Compose([self._from_numpy])\n",
        "        self.trans_labels = Compose([self._from_numpy, self._prepare_class_task])\n",
        "    def _prepare_class_task(self, tensor):\n",
        "        return ty.reshape(tensor, (-1,)).type(ty.LongTensor)\n",
        "    def __getitem__(self, index):\n",
        "        # print('here', index)\n",
        "        X = np.array(self.data[index, :])\n",
        "        y = np.array(self.lables[index])\n",
        "        if self.shuffle and type(index) == list:\n",
        "            permute = ty.randperm(len(index))\n",
        "            X = X[permute, :]\n",
        "            y = y[permute]\n",
        "        X = self.trans_data(X)\n",
        "        y = self.trans_labels(y)\n",
        "        return X, y\n",
        "\n",
        "    def _from_numpy(self, tensor):\n",
        "        return ty.from_numpy(tensor).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size"
      ],
      "metadata": {
        "id": "lFk9Zq402m1U"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creaiting samplers and two ways of sampling"
      ],
      "metadata": {
        "id": "lew5NalG9eUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both the RandomBatchSampler and loader generations are the same as the once `yousefnami` used in his artickle\n",
        "\n",
        "[Reading .h5 Files Faster with PyTorch Datasets](https://towardsdatascience.com/reading-h5-files-faster-with-pytorch-datasets-3ff86938cc)"
      ],
      "metadata": {
        "id": "vsW5vElfPhwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomBatchSampler(Sampler):\n",
        "    def __init__(self, dataset, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset_length = len(dataset)\n",
        "        self.n_batches = self.dataset_length / self.batch_size\n",
        "        self.batch_ids = ty.randperm(int(self.n_batches))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        for id in self.batch_ids:\n",
        "            idx = ty.arange(id * self.batch_size, (id + 1) * self.batch_size)\n",
        "            for index in idx:\n",
        "                yield int(index)\n",
        "        if int(self.n_batches) < self.n_batches:\n",
        "            idx = ty.arange(int(self.n_batches) * self.batch_size, self.dataset_length)\n",
        "            for index in idx:\n",
        "                yield int(index)\n",
        "\n",
        "def normal_loader(dataset, batch_size=32, drop_last=False, shuffle=True):\n",
        "    return DataLoader(dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      drop_last=drop_last,\n",
        "                      shuffle=shuffle)\n",
        "def fast_loader(dataset, batch_size=32, drop_last=False, transforms=None):\n",
        "    return DataLoader(dataset, \n",
        "                      batch_size=None,\n",
        "                      sampler=BatchSampler(RandomBatchSampler(dataset,\n",
        "                                                              batch_size),\n",
        "                                           batch_size=batch_size,\n",
        "                                           drop_last=drop_last))"
      ],
      "metadata": {
        "id": "gUkX3injPIXy"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = StockDataset(\"train_data.hdf5\")\n"
      ],
      "metadata": {
        "id": "5OnFxZblXuW8"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = normal_loader(train_data)\n",
        "train_loader_f = fast_loader(train_data)"
      ],
      "metadata": {
        "id": "y84Y1AnrY500"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "start_load = time.time()\n",
        "for i, (X,y) in enumerate(train_loader_f):\n",
        "    end_load = time.time()\n",
        "    print(i, X.shape, y.shape)\n",
        "    break\n",
        "print( f'Time taken: load({end_load - start_load:.3g}), ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFwtHNEMacE_",
        "outputId": "8c00addf-cd18-461a-da7a-c698f299123c"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([32, 128, 5]) torch.Size([32])\n",
            "Time taken: load(1.25), \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start_load = time.time()\n",
        "for i, (X,y) in enumerate(train_loader):\n",
        "    end_load = time.time()\n",
        "    print(i, X.shape, y.reshape((-1,)).shape)\n",
        "    break\n",
        "print( f'Time taken: load({end_load - start_load:.3g}), ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLeStHBpX0h6",
        "outputId": "59b74681-0edb-40f6-bab8-3fb18711f2af"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([32, 128, 5]) torch.Size([32])\n",
            "Time taken: load(1.78), \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file = tables.open_file(\"train_data.hdf5\", mode='r')\n",
        "print(train_file.root.data.shape, train_file.root.labels.shape)\n",
        "# "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2v-Qell0Vgh",
        "outputId": "0f07eaa8-680d-4384-9ba7-4a3e21e332a4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12573778, 128, 5) (12573778,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJxgtRpW-_RQ",
        "outputId": "543186b5-521f-4a20-e5c4-57a8d1fa6777"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "File(filename=train_data.hdf5, title='', mode='r', root_uep='/', filters=Filters(complevel=0, shuffle=False, bitshuffle=False, fletcher32=False, least_significant_digit=None))\n",
              "/ (RootGroup) ''\n",
              "/data (EArray(12573778, 128, 5)shuffle, blosc(5)) ''\n",
              "  atom := Float64Atom(shape=(), dflt=0.0)\n",
              "  maindim := 0\n",
              "  flavor := 'numpy'\n",
              "  byteorder := 'little'\n",
              "  chunkshape := (204, 128, 5)\n",
              "/labels (EArray(12573778,)shuffle, blosc(5)) ''\n",
              "  atom := Float64Atom(shape=(), dflt=0.0)\n",
              "  maindim := 0\n",
              "  flavor := 'numpy'\n",
              "  byteorder := 'little'\n",
              "  chunkshape := (16384,)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file.root.labels[np.array([1, 3, 45, 85,])].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9YuP10LCKSN9",
        "outputId": "3e7a1619-7fd9-4564-cc07-d29426bdb99a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4,)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file.root.data[np.array([1,3, 45, 85,]), :].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ew8-Tel3ZmV",
        "outputId": "7131db62-2a84-4ce5-d87c-a86809880b5d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 128, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_file.close()"
      ],
      "metadata": {
        "id": "tN6Dl6QZXtUe"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert('data' in train_file.root)\n",
        "assert('labels' in train_file.root)"
      ],
      "metadata": {
        "id": "n_TkjyNY3rO_"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model "
      ],
      "metadata": {
        "id": "ogBRz-QX-etj"
      }
    }
  ]
}