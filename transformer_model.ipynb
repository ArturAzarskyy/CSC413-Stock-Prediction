{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPPwp94666imcc18Lfj6bPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArturAzarskyy/CSC413-Stock-Prediction/blob/main/transformer_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer model for Stock prediction"
      ],
      "metadata": {
        "id": "9pLJEKQAho8u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preporations:\n",
        "\n",
        "Note that torch dataset  and sampler were inspired from `yousefnami`'s article about\n",
        "[Reading .h5 Files Faster with PyTorch Datasets](https://towardsdatascience.com/reading-h5-files-faster-with-pytorch-datasets-3ff86938cc)"
      ],
      "metadata": {
        "id": "JjaPnVGE9pSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Getting the pre-processed data from the "
      ],
      "metadata": {
        "id": "SlAbOtnh1p-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/amd/')"
      ],
      "metadata": {
        "id": "EunBcf6jhomO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aae0ebf0-1950-4446-d325-7faed6ecb12d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /amd/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "load_mvg_avg_f = False"
      ],
      "metadata": {
        "id": "Kq3zQAVM2S04"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if load_mvg_avg_f:\n",
        "    !cp /amd/My\\ Drive/CSC413/Data/sp_data_orig_m_avg.zip /content/\n",
        "    !unzip sp_data_orig.zip\n",
        "else:\n",
        "    !cp /amd/My\\ Drive/CSC413/Data/sp_data_orig.zip /content/\n",
        "    !unzip sp_data_orig.zip"
      ],
      "metadata": {
        "id": "YoOJi5YUjUbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a04c819d-d0e0-4968-e247-501043ec4c98"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  sp_data_orig.zip\n",
            "  inflating: test_data.hdf5          \n",
            "  inflating: train_data.hdf5         \n",
            "  inflating: val_data.hdf5           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports:"
      ],
      "metadata": {
        "id": "oVRXRA0EOSAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yEFAqEAX_5S3"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader, Sampler, BatchSampler\n",
        "from torchvision.transforms import Compose\n",
        "import tables\n",
        "import torch as ty\n",
        "import os.path\n",
        "import numpy as np\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a custom dataset for pytorch"
      ],
      "metadata": {
        "id": "sn4tkdqM1ep4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StockDataset(Dataset):\n",
        "    def __init__(self, file_name, shuffle=True):\n",
        "        super(StockDataset, self).__init__()\n",
        "        hdf5_file = tables.open_file(file_name, mode='r')\n",
        "        assert('data' in hdf5_file.root)\n",
        "        assert('labels' in hdf5_file.root)\n",
        "        self.f_name = file_name\n",
        "        self.data = hdf5_file.root.data\n",
        "        self.lables = hdf5_file.root.labels\n",
        "        self.size = self.data.shape[0]\n",
        "        self.shuffle = shuffle\n",
        "        self.trans_data = Compose([self._from_numpy])\n",
        "        self.trans_labels = Compose([self._from_numpy, self._prepare_class_task])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # print('here', index)\n",
        "        X = np.array(self.data[index, :])\n",
        "        y = np.array(self.lables[index])\n",
        "        if self.shuffle and type(index) == list:\n",
        "            permute = ty.randperm(len(index))\n",
        "            X = X[permute, :]\n",
        "            y = y[permute]\n",
        "            y = self.trans_labels(y)\n",
        "        else:\n",
        "            y = self.trans_data(y)\n",
        "        X = self.trans_data(X)\n",
        "        return X, y\n",
        "\n",
        "    def _prepare_class_task(self, tensor):\n",
        "        return ty.reshape(tensor, (-1,))\n",
        "\n",
        "    def _from_numpy(self, tensor):\n",
        "        return ty.from_numpy(tensor).float()\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size"
      ],
      "metadata": {
        "id": "lFk9Zq402m1U"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creaiting samplers and two ways of sampling"
      ],
      "metadata": {
        "id": "lew5NalG9eUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TODO  --> check if we actually do need that**\n",
        "\n",
        "\n",
        "\n",
        "Both the RandomBatchSampler and loader generations are the same as the once `yousefnami` used in his artickle\n",
        "[Reading .h5 Files Faster with PyTorch Datasets](https://towardsdatascience.com/reading-h5-files-faster-with-pytorch-datasets-3ff86938cc)"
      ],
      "metadata": {
        "id": "vsW5vElfPhwI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomBatchSampler(Sampler):\n",
        "    def __init__(self, dataset, batch_size):\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset_length = len(dataset)\n",
        "        self.n_batches = self.dataset_length / self.batch_size\n",
        "        self.batch_ids = ty.randperm(int(self.n_batches))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.batch_size\n",
        "\n",
        "    def __iter__(self):\n",
        "        for id in self.batch_ids:\n",
        "            idx = ty.arange(id * self.batch_size, (id + 1) * self.batch_size)\n",
        "            for index in idx:\n",
        "                yield int(index)\n",
        "        if int(self.n_batches) < self.n_batches:\n",
        "            idx = ty.arange(int(self.n_batches) * self.batch_size,\n",
        "                            self.dataset_length)\n",
        "            for index in idx:\n",
        "                yield int(index)\n",
        "\n",
        "def normal_loader(dataset, batch_size=32, drop_last=False, shuffle=True):\n",
        "    return DataLoader(dataset,\n",
        "                      batch_size=batch_size,\n",
        "                      drop_last=drop_last,\n",
        "                      shuffle=shuffle)\n",
        "def fast_loader(dataset, batch_size=32, drop_last=False, transforms=None):\n",
        "    return DataLoader(dataset, \n",
        "                      batch_size=None,\n",
        "                      sampler=BatchSampler(RandomBatchSampler(dataset,\n",
        "                                                              batch_size),\n",
        "                                           batch_size=batch_size,\n",
        "                                           drop_last=drop_last))"
      ],
      "metadata": {
        "id": "gUkX3injPIXy"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = StockDataset(\"train_data.hdf5\")"
      ],
      "metadata": {
        "id": "5OnFxZblXuW8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = normal_loader(train_data)\n",
        "train_loader_f = fast_loader(train_data)"
      ],
      "metadata": {
        "id": "y84Y1AnrY500"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_load = time.time()\n",
        "for i, (X,y) in enumerate(train_loader_f):\n",
        "    end_load = time.time()\n",
        "    print(i, X.shape, y.shape)\n",
        "    break\n",
        "print( f'Time taken: load({end_load - start_load:.3g}), ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFwtHNEMacE_",
        "outputId": "ef954938-e871-4530-da19-4e357f866474"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([32, 128, 5]) torch.Size([32])\n",
            "Time taken: load(0.756), \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "start_load = time.time()\n",
        "for i, (X,y) in enumerate(train_loader):\n",
        "    end_load = time.time()\n",
        "    print(i, X.shape, y.shape)\n",
        "    break\n",
        "print( f'Time taken: load({end_load - start_load:.3g}), ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLeStHBpX0h6",
        "outputId": "66bb15a2-79ec-4f95-e427-ad6668bd00c4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 torch.Size([32, 128, 5]) torch.Size([32])\n",
            "Time taken: load(1.06), \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_data = StockDataset(\"train_data.hdf5\")\n",
        "val_data = StockDataset(\"val_data.hdf5\")\n",
        "test_data = StockDataset(\"test_data.hdf5\")\n",
        "\n",
        "train_loader = normal_loader(train_data, batch_size=batch_size)\n",
        "# train_loader_f = fast_loader(train_data, batch_size=batch_size)\n",
        "val_loader = normal_loader(val_data, batch_size=batch_size)\n",
        "# val_loader_f = fast_loader(val_data, batch_size=batch_size)\n",
        "test_loader = normal_loader(test_data, batch_size=batch_size)\n",
        "# test_loader_f = fast_loader(test_data, batch_size=batch_size)"
      ],
      "metadata": {
        "id": "a7snqZLiOGIj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model "
      ],
      "metadata": {
        "id": "Vpq8FNnJN-o-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "RdqEWXgYOFfa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}